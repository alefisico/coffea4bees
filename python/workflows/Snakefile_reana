from datetime import datetime

EOS_OUTPUT = f"/eos/user/a/algomez/work/HH4b/reana/{datetime.now().strftime('%Y%m%d')}_{config['hash']}/"

#
# rule all sets the entire workflow. This is were you define the last output of the workflow.
# Snakemake will go backawrds and check what rules does the workflow need to get the output.
# #
rule final_rule:
    input:
        "output/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf",
        "output/datacards/systs/SvB_MA_ps_hh_nominal.pdf",
        "output/datacards/plots/SvB_MA_postfitplots_prefit.pdf",
        'output/datacards/impacts_combine_SvB_MA_exp_HH.pdf',
        "output/datacards/systbreakdown_SvB_MA_breakdown.pdf",
        # "output/datacards/kappa_scan.pdf"   ### this does not work yet
        # expand("output/singlefiles/histNoJCM__{sample}-{year}.coffea", sample=config['dataset'], year=config['year'])  ### trick to define wildcards
    container: config["analysis_container"]
    resources:
        voms_proxy=True,
        kerberos=True,
        unpacked_img=True,
    shell: 
        """
        cp gitdiff.txt output/
        echo "Copying output to cernbox"
        python python/base_class/php_plots/pb_deploy_plots.py output/ {EOS_OUTPUT} -r -c -j 4
        mkdir -p {EOS_OUTPUT}
        cp -r output/* {EOS_OUTPUT}
        """

rule analysis:
    output: "output/singlefiles/histNoJCM__{sample}-{year}.coffea"
    container: config["analysis_container"]
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{sample}",
        iy="{year}",
        output="histNoJCM__{sample}-{year}.coffea",
        output_path="output/singlefiles/",
        logname="histNoJCM__{sample}-{year}",
        metadata="analysis/metadata/HH4b_noJCM.yml",
        datasets=config['dataset_location']
    resources:
        voms_proxy=True,
        kerberos=True,
        unpacked_img=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
        cd python/ 
        #echo "Blinding SR region"
        #sed -i 's/blind.*/blind: true/' {params.metadata}
        echo "Running with this metadata file"
        cat {params.metadata}
        echo "Running {params.isam} {params.iy} - output {output}"
        mprof run -C -o mprofile_{params.logname}.dat python runner.py -d {params.isam} -p analysis/processors/processor_HH4b.py -y {params.iy} -o {params.output} -op ../{params.output_path} -m {params.datasets} --githash {params.hash} --gitdiff {params.diff} -c {params.metadata} 
        cd ../
        mkdir -p output/performance/
        mprof plot -o output/performance/mprofile_{params.logname}.png python/mprofile_{params.logname}.dat
        """

rule merging_coffea_files:
    input: expand(['output/singlefiles/histNoJCM__{sample}-{year}.coffea'], sample=config['dataset'], year=config['year']) 
    output: "output/histNoJCM.coffea"
    container: config["analysis_container"]
    params:
        output= "histNoJCM.coffea",
        logname= "histNoJCM"
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
        echo "Merging all the coffea files"
        mprof run -C -o python/mprofile_merge_{params.logname}.dat python python/analysis/tools/merge_coffea_files.py -f {input} -o {output}
        echo "Making a plot of the performance"
        mprof plot -o output/performance/mprofile_merge_{params.logname}.png python/mprofile_merge_{params.logname}.dat
        """

rule make_JCM:
    input: "output/histNoJCM.coffea"
    output: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    container: config["analysis_container"]
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        unpacked_img=True,
    shell:
        """
        cd python/
        echo "Computing JCM"
        python analysis/make_jcm_weights.py -o ../output/JCM/ -c passPreSel -r SB -i ../{input} -w reana
        ls ../output/JCM/
        echo "Modifying metadata file"
        sed -i 's|JCM.*|JCM: ../output/JCM/jetCombinatoricModel_SB_reana.yml|' analysis/metadata/HH4b.yml
        cat analysis/metadata/HH4b.yml
        """


### In the next rules, the input is commented out to not run JCM again. 
use rule analysis as analysis_databkgs with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/hist__{sample}-{year}.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{sample}",
        iy="{year}",
        output="hist__{sample}-{year}.coffea",
        output_path="output/singlefiles/",
        logname="hist__{sample}_{year}",
        datasets=config['dataset_location'],
        metadata="analysis/metadata/HH4b.yml",
        # metadata="analysis/metadata/HH4b_rerun_SvB.yml",

use rule analysis as analysis_signals with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/histsignal__{sample_signal}-{year}.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{sample_signal}",
        iy="{year}",
        output="histsignal__{sample_signal}-{year}.coffea",
        output_path="output/singlefiles/",
        logname="histsignal__{sample_signal}-{year}",
        datasets=config['dataset_location'],
        metadata="analysis/metadata/HH4b.yml",
        # metadata="analysis/metadata/HH4b_rerun_SvB.yml",

use rule analysis as analysis_signals_HH4b with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/histsignalHH4b__GluGluToHHTo4B_cHHH1-{year}.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="GluGluToHHTo4B_cHHH1",
        iy="{year}",
        output="histsignalHH4b__GluGluToHHTo4B_cHHH1-{year}.coffea",
        output_path="output/singlefiles/",
        logname="histsignalHH4b__GluGluToHHTo4B_cHHH1-{year}",
        datasets=config['dataset_location'],
        metadata="analysis/metadata/HH4b_signals.yml",


use rule analysis as analysis_mixedbkg_data3b with:
    output: "output/histMixedBkg_data_3b_for_mixed.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="data_3b_for_mixed",
        iy=config['year_preUL'],
        output="histMixedBkg_data_3b_for_mixed.coffea",
        output_path="output/",
        logname="mixedbkg_data3b",
        metadata="analysis/metadata/HH4b_mixed_data.yml",
        datasets=config['dataset_location']


use rule analysis as analysis_mixedbkg with:
    output: "output/histMixedBkg_TT.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=config['dataset_for_mixed'],
        iy=config['year'],
        output="histMixedBkg_TT.coffea",
        output_path="output/",
        logname="mixedbkg_TT",
        metadata="analysis/metadata/HH4b_nottcheck.yml",
        datasets=config['dataset_location']


use rule analysis as analysis_mixeddata with:
    output: "output/histMixedData.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="mixeddata",
        iy=config['year_preUL'],
        output="histMixedData.coffea",
        output_path="output/",
        logname="mixeddata",
        metadata="analysis/metadata/HH4b_nottcheck.yml",
        datasets=config['dataset_location']


use rule analysis as analysis_systematics with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/histsyst_{samplesyst}-{iysyst}.coffea"
    container: config["analysis_container"]
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{samplesyst}",
        iy="{iysyst}",
        output="histsyst_{samplesyst}-{iysyst}.coffea",
        output_path="output/singlefiles/",
        logname="syst_{samplesyst}-{iysyst}",
        metadata="analysis/metadata/HH4b_systematics.yml",
        datasets=config['dataset_location']


use rule merging_coffea_files as merging_coffea_files_syst with:
    input: expand(['output/singlefiles/histsyst_{idatsyst}-{iyear}.coffea'], idatsyst=config['dataset_systematics'], iyear=config['year']) 
    output: "output/histAll_signals_cHHHX.coffea"
    params:
        output= "histAll_signals_cHHHX.coffea",
        logname= "signals_cHHHX"

use rule merging_coffea_files as merging_coffea_files_histAll with:
    input: expand(['output/singlefiles/histsignal__{sample_signal}-{year}.coffea'], sample_signal=config['dataset_signals'], year=config['year']) + expand(['output/singlefiles/hist__{idat}-{iyear}.coffea'], idat=config['dataset'], iyear=config['year']) + expand(['output/singlefiles/histsignalHH4b__GluGluToHHTo4B_cHHH1-{iyear}.coffea'], iyear=config['year'])
    output: "output/histAll.coffea"
    params:
        output= "histAll.coffea",
        logname= "histAll"

rule make_plots:
    input: "output/histAll.coffea"
    output: "output/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf"
    container: config["analysis_container"]
    resources:
        kerberos=True,
        unpacked_img=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
        cd python/ 
        echo "Making plots"
        mprof run -C -o mprofile_makeplots.dat python plots/makePlots.py ../output/histAll.coffea -o ../output/plots/ -m plots/metadata/plotsAll.yml -s xW FvT
        echo "Making a plot of the performance"
        mprof plot -o ../output/performance/mprofile_makeplots.png mprofile_makeplots.dat
        """
        
rule convert_hist_to_json:
    input:
        inall = "output/histAll.coffea",
        insystcHHHX = "output/histAll_signals_cHHHX.coffea",
        inmixdata3b = "output/histMixedBkg_data_3b_for_mixed.coffea",
        inmixbkgtt = "output/histMixedBkg_TT.coffea",
        inmixdata = "output/histMixedData.coffea",
    output:
        outall = "output/histAll.json",
        outsystcHHHX = "output/histAll_signals_cHHHX.json",
        outmixdata3b = "output/histMixedBkg_data_3b_for_mixed.json",
        outmixbkgtt = "output/histMixedBkg_TT.json",
        outmixdata = "output/histMixedData.json",
    container: config["analysis_container"]
    resources:
        unpacked_img=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
        python python/stats_analysis/convert_hist_to_json.py -o {output.outall} -i {input.inall}
        python python/stats_analysis/convert_hist_to_json.py -o {output.outsystcHHHX} -i {input.insystcHHHX} -s
        python python/stats_analysis/convert_hist_to_json_closure.py -o {output.outmixdata3b} -i {input.inmixdata3b}
        python python/stats_analysis/convert_hist_to_json_closure.py -o {output.outmixbkgtt} -i {input.inmixbkgtt}
        python python/stats_analysis/convert_hist_to_json_closure.py -o {output.outmixdata} -i {input.inmixdata}
        """

rule convert_json_to_root:
    input: 
        file_TT = "output/histMixedBkg_TT.json",
        file_mix = "output/histMixedData.json",
        file_sig = "output/histAll.json",
        file_data3b = "output/histMixedBkg_data_3b_for_mixed.json"
    container: config["combine_container"]
    output: 
        "output/histMixedBkg_TT.root",
        "output/histMixedData.root",
        "output/histAll.root",
        "output/histMixedBkg_data_3b_for_mixed.root"
    resources:
        unpacked_img=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_TT} --output output/
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_mix} --output output/
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_sig} --output output/
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_data3b} --output output/
        """

rule run_two_stage_closure:
    input: 
        file_TT = "output/histMixedBkg_TT.root",
        file_mix = "output/histMixedData.root",
        file_sig = "output/histAll.root",
        file_data3b = "output/histMixedBkg_data_3b_for_mixed.root"
    container: config["combine_container"]
    output: "output/closureFits/ULHH_kfold/3bDvTMix4bDvT/SvB_MA/rebin1/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin1.pkl"
    params:
        outputPath = "output/closureFits/ULHH_kfold",
        rebin = "1",
        variable = "SvB_MA_ps_hh"
    resources:
        unpacked_img=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        cd python/
        python3 stats_analysis/runTwoStageClosure.py  \
            --var {params.variable} --rebin {params.rebin} --use_kfold \
            --outputPath ../{params.outputPath} \
            --input_file_TT ../{input.file_TT} \
            --input_file_mix ../{input.file_mix} \
            --input_file_sig ../{input.file_sig} \
            --input_file_data3b ../{input.file_data3b}
        """

rule make_combine_inputs:
    input:
        injson = "output/histAll.json",
        injsonsyst = "output/histAll_signals_cHHHX.json",
        bkgsyst = "output/closureFits/ULHH_kfold/3bDvTMix4bDvT/SvB_MA/rebin1/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin1.pkl"
    output:
        "output/datacards/shapes.root",
        "output/datacards/datacard_HHbb_2018.txt"
    container: config["combine_container"]
    params:
        variable= "SvB_MA.ps_hh",
        rebin=1
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        echo "Making combine inputs with full stats"
        python3 python/stats_analysis/make_combine_inputs.py \
            --var {params.variable} \
            -f {input.injson} \
            --syst_file {input.injsonsyst} \
            --bkg_syst_file {input.bkgsyst} \
            --output_dir output/datacards/ \
            --rebin {params.rebin} \
            --metadata python/stats_analysis/metadata/HH4b.yml
        echo "Making combine inputs stat only"
        python3 python/stats_analysis/make_combine_inputs.py \
            --var {params.variable} \
            -f {input.injson} \
            --syst_file {input.injsonsyst} \
            --bkg_syst_file {input.bkgsyst} \
            --output_dir output/datacards/stat_only/ \
            --rebin {params.rebin} \
            --metadata python/stats_analysis/metadata/HH4b.yml \
            --stat_only
        """

rule run_combine:
    input: "output/datacards/datacard_HHbb_2018.txt"
    output: 
        "output/datacards/datacard.root",
        "output/datacards/datacard.txt"
    container: config["combine_container"]
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        cat {input}
        echo "RUNNING COMBINE"
        source python/stats_analysis/run_combine.sh output/datacards/ --limits
        echo "RUNNING COMBINE STAT ONLY"
        source python/stats_analysis/run_combine.sh output/datacards/stat_only --limits
        """

rule run_impacts:
    input: "output/datacards/datacard.root"
    output: "output/datacards/impacts_combine_SvB_MA_exp_HH.pdf"
    container: config["combine_container"]
    resources:
        voms_proxy=True,
        kerberos=True,
        unpacked_img=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        cat {input}
        echo "RUNNING COMBINE"
        source python/stats_analysis/run_combine.sh output/datacards/ --impacts
        """

rule run_postfit:
    input: "output/datacards/datacard.root"
    output: "output/datacards/plots/SvB_MA_postfitplots_prefit.pdf"
    container: config["combine_container"]
    params:
        output_dir="output/datacards/plots/"
    resources:
        voms_proxy=True,
        kerberos=True,
        unpacked_img=True,
        compute_backend="kubernetes"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        cd python/
        cat {input}
        echo "RUNNING COMBINE"
        source stats_analysis/run_combine.sh ../output/datacards/ --postfit
        python3 plots/make_postfit_plot.py -i ../output/datacards/fitDiagnostics_SvB_MA_prefit_sb.root -o ../{params.output_dir} -t prefit
        python3 plots/make_postfit_plot.py -i ../output/datacards/fitDiagnostics_SvB_MA_prefit_sb.root -o ../{params.output_dir} -t fit_b
        python3 plots/make_postfit_plot.py -i ../output/datacards/fitDiagnostics_SvB_MA_prefit_sb.root -o ../{params.output_dir} -t fit_s
        """

rule make_syst_plots:
    input: 
        root="output/datacards/shapes.root",
        datacard="output/datacards/datacard.txt"
    output: "output/datacards/systs/SvB_MA_ps_hh_nominal.pdf"
    container: config["combine_container"]
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        unpacked_img=True,
        kubernetes_memory_limit="8Gi"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        cd python/
        echo "Making syst plots"
        python3 plots/make_syst_plots.py -i ../{input.root} -o ../output/datacards/systs/ -d ../{input.datacard} --variable SvB_MA_ps_hh
        """

rule make_syst_breakdown:
    input: "output/datacards/datacard.root"
    output: "output/datacards/systbreakdown_SvB_MA_breakdown.pdf"
    container: config["combine_container"]
    params:
        output_dir="output/datacards/plots/"
    resources:
        voms_proxy=True,
        kerberos=True,
        unpacked_img=True,
        compute_backend="kubernetes"
    shell:
        """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        cd python/
        cat {input}
        echo "RUNNING COMBINE"
        source stats_analysis/run_combine.sh ../output/datacards/ --systbreakdown
        """

# rule kappa_scan:
#     input: "output/datacards/datacard.root"
#     output: "output/datacards/kappa_scan.pdf"
#     container: "docker://docker.io/cmssw/el7:aarch64"
#     resources:
#         voms_proxy=True,
#         kerberos=True,
#         unpacked_img=True,
#         compute_backend="kubernetes"
#     shell:
#         """
#         cd python/stats_analysis/inference/
#         bash setup.sh
#         law run PlotUpperLimits --version dev --datacards output/datacards/datacard.txt --xsec fb --y-log --scan-parameters kl,20,20,5
#         cp data/store/PlotUpperLimits/hh_model__model_default/datacards_716fa319cb/m125.0/poi_r/dev/limits__poi_r__scan_kl_20.0_20.0_n5__params_r_gghh1.0_r_qqhh1.0_kt1.0_CV1.0_C2V1.0__fb_log.pdf ../../../output/datacards/kappa_scan.pdf
#         """
